{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vTJclL94jbW1"
      },
      "source": [
        "# Exercise 5 : Neural machine Translation with `nn.Transformer` and Multi30k dataset\n",
        "========================================================\n",
        "\n",
        "**USI, Deep Learning lab SA 2025-2026**\n",
        "\n",
        "Lecturer: Eleonora Vercesi. TAs: Alvise Dei Rossi, Stefano Huber, Lorenzo Scarciglia\n",
        "\n",
        "========================================================\n",
        "\n",
        "In this exercise we're going to tackle a \"simple\" (toy) machine translation task with a full transformer model, implemented with Pytorch.\n",
        "We will use the [Multi30k dataset from HuggingFace](https://huggingface.co/datasets/bentrevett/multi30k) that\n",
        "yields a pair of source-target raw sentences (source: German, target: English). Originally this dataset was introduced by researchers to stimulate multilingual multimodal research (sentences are image descriptions, [link to article](https://arxiv.org/abs/1605.00459)). It includes approximately 30 thousand sentence-pairs, hence the name.\n",
        "\n",
        "In this example, we show how to load the dataset, tokenize raw text sentences,\n",
        "build vocabulary, and numericalize tokens into tensor. We'll then built the transformer model and train it onto the processed data.\n",
        "\n",
        "For comparison the original Trasformer model (which is not too far with respect to the implementation we'll see today) discussed in the paper \"[Attention is all you need](https://user.phil.hhu.de/~cwurm/wp-content/uploads/2020/01/7181-attention-is-all-you-need.pdf)\" was instead trained on the WMT 2014 English-German dataset consisting of 4.5M sentence pairs, with 37k tokens and on 8 NVIDIA P100 GPUs for 12 hours."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pASTPAuooV-r"
      },
      "source": [
        "# Libraries version & Tokenizer download\n",
        "===================="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ml5y3WPEsD20"
      },
      "source": [
        "**Note**: You **SHOULD definitely** use a GPU for this exercise; we'll use Colab GPUs but if you finished your GPU time on Colab, you can also run this notebook on Kaggle or Lightning.\n",
        "\n",
        "The following versioning configuration will throw an error but this shouldn't be relevant for running the rest of the notebook. Keep it unchanged."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "StUb8lYOtZdk"
      },
      "source": [
        "In this notebook we'll download the tokenizer, focusing more on the architecture of the model; but of course you could tokenize the sentences with custom tokenizers.\n",
        "\n",
        "We'll leverage one tokenizer for each language. More in detail, we leverage the original BERT tokenizer to tokenize the English sentences (https://huggingface.co/google-bert/bert-base-cased), and the German version to tokenize the German sentences (https://huggingface.co/google-bert/bert-base-german-cased)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mfw-1VjhhK4g",
        "outputId": "8a972565-54a9-43b7-edef-26c561a54be6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu126\n",
            "Requirement already satisfied: torch==2.9.0 in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.24.0+cu126)\n",
            "Requirement already satisfied: transformers==4.57.1 in /usr/local/lib/python3.12/dist-packages (4.57.1)\n",
            "Requirement already satisfied: datasets==4.0.0 in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0) (3.5.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.57.1) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.57.1) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.57.1) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers==4.57.1) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.57.1) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers==4.57.1) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.57.1) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers==4.57.1) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers==4.57.1) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets==4.0.0) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets==4.0.0) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets==4.0.0) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets==4.0.0) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets==4.0.0) (0.70.16)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets==4.0.0) (3.13.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers==4.57.1) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.57.1) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.57.1) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.57.1) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.57.1) (2025.11.12)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch==2.9.0) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.9.0) (3.0.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets==4.0.0) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets==4.0.0) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets==4.0.0) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==4.0.0) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==4.0.0) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==4.0.0) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==4.0.0) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==4.0.0) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==4.0.0) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==4.0.0) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets==4.0.0) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip3 install torch==2.9.0 torchvision --index-url https://download.pytorch.org/whl/cu126 transformers==4.57.2 datasets==4.0.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITYDKys4o5XI"
      },
      "source": [
        "Data Downloading and Processing\n",
        "============================"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y2UKEEhqxA7V",
        "outputId": "b1f20bc4-d903-4b35-d0e3-5a0afbd95442"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['en', 'de'],\n",
            "        num_rows: 29000\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['en', 'de'],\n",
            "        num_rows: 1014\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['en', 'de'],\n",
            "        num_rows: 1000\n",
            "    })\n",
            "})\n",
            "Example: {'en': 'Two young, White males are outside near many bushes.', 'de': 'Zwei junge weiÃŸe MÃ¤nner sind im Freien in der NÃ¤he vieler BÃ¼sche.'}\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load Multi30k from Hugging Face\n",
        "hf_multi30k = load_dataset(\"bentrevett/multi30k\")\n",
        "\n",
        "print(hf_multi30k)\n",
        "\n",
        "example = hf_multi30k[\"train\"][0]\n",
        "\n",
        "SRC_LANGUAGE = 'de'\n",
        "TGT_LANGUAGE = 'en'\n",
        "\n",
        "print(\"Example:\", example)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sNsRMNzsueZQ"
      },
      "source": [
        "We're going first to get the tokenizers and vocabularies. We will use the pre-trained tokenizers from the English (the original) and German versions of BERT. These tokenizers use [WordPiece algorithm](https://huggingface.co/learn/llm-course/chapter6/6) to split words into sub-words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4xSHCIhLKhCa",
        "outputId": "4e5afb0d-a381-4aad-993f-b166777e4cb1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original tokenizer special map\n",
            "[SRC] tokenizer {'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}\n",
            "[TGT] tokenizer {'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}\n",
            "\n",
            "New tokenizer special map\n",
            "[SRC] tokenizer {'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}\n",
            "[TGT] tokenizer {'bos_token': '[BOS]', 'eos_token': '[EOS]', 'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer\n",
        "tokenizer_src = AutoTokenizer.from_pretrained('google-bert/bert-base-german-cased')\n",
        "tokenizer_tgt = AutoTokenizer.from_pretrained('google-bert/bert-base-cased')\n",
        "\n",
        "# We want to have the following special tokens: [UNK], [PAD], [BOS], [EOS].\n",
        "# More in detail, BOS and EOS tokens are necessary in the decoding step. The former is the first token, while the latter is the last token produced in the autoregressive mode.\n",
        "# Check whether if we have them, and manually add the remaining ones\n",
        "print(\"Original tokenizer special map\")\n",
        "print(\"[SRC] tokenizer\", tokenizer_src.special_tokens_map)\n",
        "print(\"[TGT] tokenizer\", tokenizer_tgt.special_tokens_map)\n",
        "tokenizer_tgt.add_special_tokens({'eos_token':'[EOS]', 'bos_token':'[BOS]'}) # Special tokens need to match the keys in the list of predefined special attributes https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizer.add_special_tokens.special_tokens_dict\n",
        "print(\"\\nNew tokenizer special map\")\n",
        "print(\"[SRC] tokenizer\", tokenizer_src.special_tokens_map) # No changes\n",
        "print(\"[TGT] tokenizer\", tokenizer_tgt.special_tokens_map)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "QlKmkE7_xA7X"
      },
      "outputs": [],
      "source": [
        "# Once we have updated the vocabularies, we can retrieve the vocabularies\n",
        "vocab_src = tokenizer_src.get_vocab()\n",
        "vocab_tgt = tokenizer_tgt.get_vocab()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9FRVT2Syt__Y"
      },
      "source": [
        "Let's make a couple examples:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L7v8tjUbt-jR",
        "outputId": "4389fde7-94a7-4271-986d-22519e3eae15"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[SRC] vocab size: 30000\n",
            "[TGT] vocab size: 28998\n",
            "\n",
            "[SRC]\n",
            "'rotes' is not present in the vocabulary. However, the tokens are the following: ['rot', '##es']\n",
            "Auto: 3512\n",
            "\n",
            "[TGT]\n",
            "red: 1894\n",
            "Car: 8185\n"
          ]
        }
      ],
      "source": [
        "print(f\"[SRC] vocab size: {len(vocab_src)}\")\n",
        "print(f\"[TGT] vocab size: {len(vocab_tgt)}\")\n",
        "src_terms = ['rotes', 'Auto']\n",
        "tgt_terms = ['red', 'Car']\n",
        "print('\\n[SRC]')\n",
        "for s in src_terms:\n",
        "    try:\n",
        "        print(f\"{s}: {vocab_src[s]}\")\n",
        "    except Exception as e:\n",
        "        print(f\"'{s}' is not present in the vocabulary. However, the tokens are the following: {tokenizer_tgt.tokenize(s)}\")\n",
        "print('\\n[TGT]')\n",
        "for t in tgt_terms:\n",
        "    try:\n",
        "        print(f\"{t}: {vocab_tgt[t]}\")\n",
        "    except Exception as e:\n",
        "        print(f\"'{t}' is not present in the vocabulary. However, the tokens are the following: {tokenizer_tgt.tokenize(t)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LCasWo-VxA7Y",
        "outputId": "d4e2bf72-297a-4647-eb8e-9bee7342ca0c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[SRC] {'[UNK]': 2, '[SEP]': 4, '[PAD]': 0, '[CLS]': 3, '[MASK]': 5}\n",
            "[TGT] {'[BOS]': 28997, '[EOS]': 28996, '[UNK]': 100, '[SEP]': 102, '[PAD]': 0, '[CLS]': 101, '[MASK]': 103}\n"
          ]
        }
      ],
      "source": [
        "# Save the special token mapping\n",
        "special_token_mapping_src = {}\n",
        "for i,j in zip(tokenizer_src.all_special_ids, tokenizer_src.all_special_tokens):\n",
        "    special_token_mapping_src[j] = i\n",
        "special_token_mapping_tgt = {}\n",
        "for i,j in zip(tokenizer_tgt.all_special_ids, tokenizer_tgt.all_special_tokens):\n",
        "    special_token_mapping_tgt[j] = i\n",
        "\n",
        "print('[SRC]', special_token_mapping_src)\n",
        "print('[TGT]', special_token_mapping_tgt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "ER34b8tkxA7Y"
      },
      "outputs": [],
      "source": [
        "PAD_IDX_SRC = special_token_mapping_src['[PAD]'] # tokenizer_src.pad_token_id\n",
        "UNK_IDX_SRC = special_token_mapping_src['[UNK]'] # tokenizer_src.unk_token_id\n",
        "\n",
        "PAD_IDX_TGT = special_token_mapping_tgt['[PAD]'] # tokenizer_tgt.pad_token_id\n",
        "UNK_IDX_TGT = special_token_mapping_tgt['[UNK]'] # tokenizer_tgt.unk_token_id\n",
        "BOS_IDX_TGT = special_token_mapping_tgt['[BOS]'] # tokenizer_tgt.bos_token_id\n",
        "EOS_IDX_TGT = special_token_mapping_tgt['[EOS]'] # tokenizer_tgt.eos_token_id\n",
        "\n",
        "PAD_IDX = PAD_IDX_TGT # Used by the loss function\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z3IbsplcxA7Y",
        "outputId": "a6bebe0d-94b5-4391-90be-7cec5bec5f31"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(28997, 28996, 0)"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "BOS_IDX_TGT, EOS_IDX_TGT, PAD_IDX"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fpTngYZHKhCa"
      },
      "source": [
        "Encoder Decoder Transformer Network\n",
        "=================================\n",
        "\n",
        "Transformer is a [sequence to sequence](https://proceedings.neurips.cc/paper/2014/hash/a14ac55a4f27472c5d894ec1c3c743d2-Abstract.html) model introduced in [\"Attention is all you\n",
        "need\"](https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)\n",
        "paper for machine translation tasks.\n",
        "\n",
        "<img src=\"https://www.jeremyjordan.me/content/images/2023/05/multi-head-attention.png\" height=500>\n",
        "<img src=\"https://www.jeremyjordan.me/content/images/2023/05/transformer-architecture.png\" height=700>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wYmy_GcMQ9QJ"
      },
      "source": [
        "## Embedding and positional encoding\n",
        "\n",
        "Below, we will create a\n",
        "Seq2Seq network that uses Transformer. The network consists of three\n",
        "parts. First part is the embedding layer. This layer converts tensor of\n",
        "input indices into corresponding tensor of input embeddings. These\n",
        "embedding are further augmented with positional encodings to provide\n",
        "position information of input tokens to the model (more details [here](https://kazemnejad.com/blog/transformer_architecture_positional_encoding/)).\n",
        "\n",
        "<img src=\"https://jalammar.github.io/images/t/transformer_positional_encoding_example.png\" width=800 height=300>\n",
        "\n",
        "In the following image Depth is the embedding size.\n",
        "\n",
        "<img src=\"https://kazemnejad.com/img/transformer_architecture_positional_encoding/positional_encoding.png\" width=800 height=350>\n",
        "\n",
        "We report here the original implementation\n",
        "\n",
        "\n",
        "\\begin{equation}\n",
        "PE_{(pos, 2i)} = \\sin\\left(\\frac{pos}{10000^{\\frac{2i}{d_{model}}}}\\right)\n",
        "\\end{equation}\n",
        "\n",
        "\\begin{equation}\n",
        "PE_{(pos, 2i+1)} = \\cos\\left(\\frac{pos}{10000^{\\frac{2i}{d_{model}}}}\\right)\n",
        "\\end{equation}\n",
        "\n",
        "Try to implement a Pytorch layer following the equations above. Consider that it must match the dimension of the Embedding Layer reported below.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HxuLdLhuRYuJ"
      },
      "outputs": [],
      "source": [
        "from torch import Tensor\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import Transformer\n",
        "import math\n",
        "\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    pass # TODO\n",
        "\n",
        "\n",
        "# helper Module to convert tensor of input indices into corresponding tensor of token embeddings\n",
        "class TokenEmbedding(nn.Module):\n",
        "    def __init__(self, vocab_size: int, emb_size):\n",
        "        super(TokenEmbedding, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
        "        self.emb_size = emb_size\n",
        "\n",
        "    def forward(self, tokens: Tensor):\n",
        "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98DxpAzJRSAI"
      },
      "source": [
        "## Encoder and decoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZ4Dy5jevDHH"
      },
      "source": [
        "\n",
        "Embedding layers and positional encoding are used both in the Encoder and Decoder part of the Transformer architecture.\n",
        "\n",
        "The key difference wrt recurrent networks is the attention mechanism, also present in both encoder and decoder.\n",
        "\n",
        "<img src=\"https://miro.medium.com/v2/resize:fit:1100/format:webp/1*ArTXQZip_TwbU6gLshXOEw.png\">\n",
        "\n",
        "For each token we create a Query vector $q$ a key vector $k$ and a value vector $v$, through linear transformation. Stacking these vectors we obtain the corresponding matrices $Q$, $K$, $V$.\n",
        "\n",
        "Self attention is then computed as:\n",
        "\n",
        "$$\n",
        "\\text{Attention}(Q,K,V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{\\text{d}_{k}}}\\right) V\n",
        "$$\n",
        "\n",
        "The $QK^T$ matrix multiplication computes the relevance between each pair of words (attention weights).\n",
        "\n",
        "This relevance is then used as a \"factor\" to compute the weighted sum of all the values words.\n",
        "\n",
        "This is done in parallel multiple times, for multiple heads (`nhead`). The idea is to capture different aspects of the input.\n",
        "\n",
        "Self-attention is present both in the encoder and decoder parts of the network, with some key differences:\n",
        "\n",
        "- For the encoder self-attention the input sequence pays attention to itself: masking is done only on PAD tokens, to avoid attention weight on padding.\n",
        "\n",
        "<img src=\"https://miro.medium.com/v2/resize:fit:1100/format:webp/1*026nzf4bw_DdSZ7kLJWcog.png\">\n",
        "\n",
        "- For the decoder, multi head attention is present twice:\n",
        "  - Right after positional encoding, we have an attention mechanism where the the output sequence pays attention to itself. The mask is defined so that it will prevent the model from looking into future words when making predictions (masked self-attention). Padding is also masked. Said in other words, in the Decoder, Self-attention masking serves to prevent the decoder from â€˜peekingâ€™ ahead at the rest of the target sentence when predicting the next word.\n",
        "\n",
        "<img src=\"https://miro.medium.com/v2/resize:fit:1100/format:webp/1*cawtdZLjT9hp7ByG2vcKOQ.png\">\n",
        "\n",
        "  - After the previous attention mechanism, the Q matrix arrives from it, while K and V from the Encoder. In practice, In the Encoder-Decoder Attention, the Query is obtained from the target sentence and the Key/Value from the source sentence. Thus it computes the relevance of each word in the target sentence to each word in the source sentence.\n",
        "\n",
        "\n",
        "  The masking is done as per the encoder part here, being careful about padding.\n",
        "\n",
        "<img src=\"https://miro.medium.com/v2/resize:fit:1100/format:webp/1*OYi49Pkg-Vl3D4HleEuu7g.png\">\n",
        "\n",
        "\n",
        "**Note**: not in images, but an encoded \\<EOS\\> token would be present at the end of each sequence in the encoder, while an encoded \\<BOS\\> token is present in the decoder at the start of each sequence.\n",
        "\n",
        "Keep in mind that the decoder part of the Transformers works differently when in training (teacher-forcing) and when in inference mode (autoregressive)\n",
        "\n",
        "\n",
        "\n",
        "This process is then followed by a normalization layer, skip connections, feedforward and again normalization layer + skip connection.\n",
        "\n",
        "Multiple encoder (`num_encoder_layers`) and decoder (`num_decoder_layers`) layers are stacked. **Note that dimension of the representation of the sequence is preserved through the architecture**.\n",
        "Finally, the output of the Transformer model is passed through\n",
        "a linear layer that gives unnormalized probabilities for each token in the\n",
        "target language.\n",
        "\n",
        "<img src=\"https://miro.medium.com/v2/resize:fit:1100/format:webp/1*dQTK3oeYqOBUDVgNktSpCw.png\" height=600 width=750>\n",
        "\n",
        "the [Transformer](https://docs.pytorch.org/docs/stable/generated/torch.nn.Transformer.html) model is easily implemented with its Pytorch class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "5Q0iX6omvAwV"
      },
      "outputs": [],
      "source": [
        "class Seq2SeqTransformer(nn.Module):\n",
        "    def __init__(self,\n",
        "                 num_encoder_layers: int,\n",
        "                 num_decoder_layers: int,\n",
        "                 emb_size: int,\n",
        "                 nhead: int,\n",
        "                 src_vocab_size: int,\n",
        "                 tgt_vocab_size: int,\n",
        "                 dim_feedforward: int = 512,\n",
        "                 dropout: float = 0.1,\n",
        "                 batch_first: bool = True):\n",
        "        super(Seq2SeqTransformer, self).__init__()\n",
        "        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)\n",
        "        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)\n",
        "        self.positional_encoding = PositionalEncoding(\n",
        "            emb_size, dropout=dropout)\n",
        "        self.transformer = Transformer(d_model=emb_size,\n",
        "                                       nhead=nhead,\n",
        "                                       num_encoder_layers=num_encoder_layers,\n",
        "                                       num_decoder_layers=num_decoder_layers,\n",
        "                                       dim_feedforward=dim_feedforward,\n",
        "                                       dropout=dropout,\n",
        "                                       batch_first=batch_first)\n",
        "        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n",
        "\n",
        "    def forward(self,\n",
        "                src: Tensor,\n",
        "                trg: Tensor,\n",
        "                src_mask: Tensor,\n",
        "                tgt_mask: Tensor,\n",
        "                src_padding_mask: Tensor,\n",
        "                tgt_padding_mask: Tensor,\n",
        "                memory_key_padding_mask: Tensor):\n",
        "        src_emb = self.positional_encoding(self.src_tok_emb(src)) # [B, src_seq_len, E] (Batch, src_seq_len, Embedding)\n",
        "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg)) # [B, tgt_seq_len, E]\n",
        "        outs = self.transformer(src_emb,\n",
        "                                tgt_emb,\n",
        "                                # Attention masks\n",
        "                                src_mask,\n",
        "                                tgt_mask,\n",
        "                                None, # memory mask parameter (decoder cross-attention)\n",
        "                                # Prevent attention computation on padding tokens\n",
        "                                src_padding_mask,\n",
        "                                tgt_padding_mask,\n",
        "                                memory_key_padding_mask)\n",
        "        return self.generator(outs)\n",
        "\n",
        "    def encode(self, src: Tensor, src_mask: Tensor):\n",
        "        return self.transformer.encoder(self.positional_encoding(\n",
        "                            self.src_tok_emb(src)), src_mask)\n",
        "\n",
        "    def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor):\n",
        "        return self.transformer.decoder(self.positional_encoding(\n",
        "                          self.tgt_tok_emb(tgt)), memory,\n",
        "                          tgt_mask)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZkPwEpIKhCb"
      },
      "source": [
        "Let\\'s now define the parameters of our model and instantiate the same.\n",
        "Below, we also define our loss function which is the cross-entropy loss\n",
        "and the optimizer used for training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "QwvGEqg4KhCb"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(0)\n",
        "\n",
        "SRC_VOCAB_SIZE = len(vocab_src)\n",
        "TGT_VOCAB_SIZE = len(vocab_tgt)\n",
        "EMB_SIZE = 512\n",
        "NHEAD = 8\n",
        "FFN_HID_DIM = 512\n",
        "BATCH_SIZE = 128 # ~10GB of VRAM\n",
        "NUM_ENCODER_LAYERS = 3\n",
        "NUM_DECODER_LAYERS = 3\n",
        "\n",
        "model = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE,\n",
        "                                 NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM)\n",
        "\n",
        "for p in model.parameters():\n",
        "    if p.dim() > 1:\n",
        "        nn.init.xavier_uniform_(p)\n",
        "\n",
        "model = model.to(DEVICE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KcLAEwMM15gL"
      },
      "source": [
        "Define the loss function and the optimizer. Remember, the loss should ignore padding; take a look at the Pytorch documentation of the appropriate loss (which one should you use for multi-class classification?) in order to understand how to do it.\n",
        "\n",
        "Also, how would you check if your model is too large or if everything you expect is within the model?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1qyxBr1X145q"
      },
      "outputs": [],
      "source": [
        "loss_fn = # TODO\n",
        "\n",
        "optimizer = # TODO\n",
        "\n",
        "# TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDPYAvUtKhCb"
      },
      "source": [
        "Collation\n",
        "=========\n",
        "\n",
        "As seen in the `Data Downloading and Processing` section, our data iterator yields a pair of raw strings.\n",
        "We need to convert these string pairs into the batched tensors that can be processed by our `Seq2Seq` network defined previously.\n",
        "Below we define our collate function that converts a batch of raw strings into batch tensors that can be fed directly into our model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "YGmPfm1QxA7a"
      },
      "outputs": [],
      "source": [
        "def tokenize_example(example):\n",
        "    src = tokenizer_src(\n",
        "        example[SRC_LANGUAGE],\n",
        "        add_special_tokens=False) # Avoid adding CLS/SEP from the BERT tokenizer\n",
        "\n",
        "    tgt = tokenizer_tgt(\n",
        "        example[TGT_LANGUAGE],\n",
        "        add_special_tokens=False) # Avoid adding CLS/SEP from the BERT tokenizer.\n",
        "\n",
        "    # Now, we add [BOS] and [EOS]\n",
        "    tgt_ids = [tokenizer_tgt.bos_token_id] + tgt['input_ids'] + [tokenizer_tgt.eos_token_id]\n",
        "\n",
        "    return {\n",
        "        \"input_ids_src\": src['input_ids'],\n",
        "        \"input_ids_tgt\": tgt_ids,\n",
        "    }\n",
        "\n",
        "tokenized = hf_multi30k.map(tokenize_example, batched=False, remove_columns=hf_multi30k['train'].column_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PMh5mnCsxA7a",
        "outputId": "e4a7e57b-bebd-42be-860a-7f6f4483a941"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(['Zwei',\n",
              "  'junge',\n",
              "  'weiÃŸe',\n",
              "  'MÃ¤nner',\n",
              "  'sind',\n",
              "  'im',\n",
              "  'Freien',\n",
              "  'in',\n",
              "  'der',\n",
              "  'NÃ¤he',\n",
              "  'vieler',\n",
              "  'BÃ¼',\n",
              "  '##sche',\n",
              "  '.'],\n",
              " ['[BOS]',\n",
              "  'Two',\n",
              "  'young',\n",
              "  ',',\n",
              "  'White',\n",
              "  'males',\n",
              "  'are',\n",
              "  'outside',\n",
              "  'near',\n",
              "  'many',\n",
              "  'bushes',\n",
              "  '.',\n",
              "  '[EOS]'])"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "src_tokens = tokenized['train'][0]['input_ids_src']\n",
        "tgt_tokens = tokenized['train'][0]['input_ids_tgt']\n",
        "tokenizer_src.convert_ids_to_tokens(src_tokens), tokenizer_tgt.convert_ids_to_tokens(tgt_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ux7GuRXv2yi9"
      },
      "source": [
        "Given the transform helper functions above, how would you define a collate function, which will be given to the DataLoaders, to appropriately prepare batches? Remember to also pad sequences with the ``pad_sequence`` function, imported from ``torch.nn.utils.rnn``."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j_8B6HnU3Ipd"
      },
      "outputs": [],
      "source": [
        "def collate_fn(batch):\n",
        "  pass # TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eDLSoV18xA7m"
      },
      "source": [
        "# Masking functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "2DTymTONxA7m"
      },
      "outputs": [],
      "source": [
        "def create_mask(src, tgt):\n",
        "    src_seq_len = src.shape[1] # [Batch dim, Sequence length]\n",
        "    tgt_seq_len = tgt.shape[1]\n",
        "\n",
        "    tgt_mask = torch.nn.Transformer.generate_square_subsequent_mask(tgt_seq_len, device=DEVICE)\n",
        "    src_mask = torch.zeros((src_seq_len, src_seq_len),device=DEVICE).type(torch.bool)\n",
        "\n",
        "    src_padding_mask = (src == PAD_IDX_SRC) # [Batch size, sequence length]\n",
        "    tgt_padding_mask = (tgt == PAD_IDX_TGT) # [Batch size, sequence length]\n",
        "    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZiX-jn_Rbr6J"
      },
      "source": [
        "## Example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dmHA25H9ZQnp",
        "outputId": "f6f1ade2-9bcb-4640-eee9-d037d326984d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "example_src\n",
            " tensor([[9.1140, 3.8354, 4.7665, 6.8479, 3.0648],\n",
            "        [4.8960, 2.2092, 3.3374, 4.8782, 9.9941],\n",
            "        [3.6643, 3.4645, 0.4654, 1.9645, 0.1600],\n",
            "        [3.8243, 1.6387, 1.0004, 0.0000, 0.0000],\n",
            "        [3.9472, 7.0073, 9.0985, 0.0000, 0.0000]])\n",
            "example_tgt\n",
            " tensor([[2.3766, 2.5385, 2.4919, 4.1884, 2.9207],\n",
            "        [0.3512, 1.4124, 0.3562, 4.8736, 0.2855],\n",
            "        [3.0243, 3.2835, 3.6289, 2.3608, 0.0000],\n",
            "        [3.8651, 0.8942, 0.3834, 4.3596, 0.0000],\n",
            "        [4.2306, 0.1578, 1.2383, 1.9305, 0.0000]])\n"
          ]
        }
      ],
      "source": [
        "example_src = 10*torch.rand(5,5)\n",
        "example_tgt = 5*torch.rand(5,5)\n",
        "# Put some padding tokens\n",
        "example_src[3:,-2:] = PAD_IDX_SRC\n",
        "example_tgt[2:,-1:] = PAD_IDX_TGT\n",
        "\n",
        "print(\"example_src\\n\", example_src)\n",
        "print(\"example_tgt\\n\", example_tgt)\n",
        "\n",
        "# Compute the masks\n",
        "src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(example_src, example_tgt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GcisZ_v9ajds",
        "outputId": "674fd506-f34e-439e-adc9-c49617c4ebbe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "src_mask\n",
            " tensor([[False, False, False, False, False],\n",
            "        [False, False, False, False, False],\n",
            "        [False, False, False, False, False],\n",
            "        [False, False, False, False, False],\n",
            "        [False, False, False, False, False]], device='cuda:0')\n",
            "tgt_mask\n",
            " tensor([[0., -inf, -inf, -inf, -inf],\n",
            "        [0., 0., -inf, -inf, -inf],\n",
            "        [0., 0., 0., -inf, -inf],\n",
            "        [0., 0., 0., 0., -inf],\n",
            "        [0., 0., 0., 0., 0.]], device='cuda:0')\n",
            "\n",
            "Padding masks:\n",
            "src_padding_mask\n",
            " tensor([[False, False, False, False, False],\n",
            "        [False, False, False, False, False],\n",
            "        [False, False, False, False, False],\n",
            "        [False, False, False,  True,  True],\n",
            "        [False, False, False,  True,  True]])\n",
            "tgt_padding_mask\n",
            " tensor([[False, False, False, False, False],\n",
            "        [False, False, False, False, False],\n",
            "        [False, False, False, False,  True],\n",
            "        [False, False, False, False,  True],\n",
            "        [False, False, False, False,  True]])\n"
          ]
        }
      ],
      "source": [
        "print(\"src_mask\\n\", src_mask)\n",
        "print(\"tgt_mask\\n\", tgt_mask)\n",
        "print(\"\\nPadding masks:\")\n",
        "print(\"src_padding_mask\\n\", src_padding_mask)\n",
        "print(\"tgt_padding_mask\\n\", tgt_padding_mask)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vh8zmUlPcdEn"
      },
      "source": [
        "ðŸ›‘ Depending on the PyTorch version, the masking behaviour may change. Check the documentation available!\n",
        "\n",
        "In this notebook, we are running <u>torch 2.9.0</u>\n",
        "\n",
        "Documentation: https://docs.pytorch.org/docs/stable/generated/torch.nn.Transformer.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OdUkQVC2CzAt"
      },
      "source": [
        "# Training and inference functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K079UrMSKhCc"
      },
      "source": [
        "Let\\'s define training and evaluation loop that will be called for each\n",
        "epoch.\n",
        "\n",
        "Note that in the validation epoch we also compute [perplexity](https://medium.com/nlplanet/two-minutes-nlp-perplexity-explained-with-simple-probabilities-6cdc46884584).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R1_qCLFdxA7m",
        "outputId": "027cb5fd-e74c-4e71-8e5b-a194e4c094eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch keys dict_keys(['input_ids_src', 'input_ids_tgt'])\n",
            "Shapes of source and target: torch.Size([128, 51]) torch.Size([128, 49])\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    tokenized['train'],\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    collate_fn=collate_fn)\n",
        "\n",
        "valid_loader = DataLoader(\n",
        "    tokenized['validation'],\n",
        "    batch_size=BATCH_SIZE, # To make sure it fits the memory\n",
        "    shuffle=False,\n",
        "    collate_fn=collate_fn)\n",
        "\n",
        "batch = next(iter(train_loader))\n",
        "print(\"Batch keys\", batch.keys())\n",
        "print(\"Shapes of source and target:\", batch['input_ids_src'].shape, batch['input_ids_tgt'].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8zd3guSWKhCc"
      },
      "outputs": [],
      "source": [
        "def train_step(model, batch, optimizer, device, loss_fn):\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    src = batch[\"input_ids_src\"].to(device)\n",
        "    tgt = batch[\"input_ids_tgt\"].to(device)\n",
        "\n",
        "    # Shift target, teacher forcing\n",
        "    tgt_input = tgt[:, :-1]\n",
        "    tgt_output = tgt[:, 1:]\n",
        "\n",
        "    # Masks\n",
        "    src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
        "\n",
        "    # Forward pass\n",
        "    logits = model(\n",
        "        src,\n",
        "        tgt_input,\n",
        "        src_mask,\n",
        "        tgt_mask,\n",
        "        src_padding_mask,\n",
        "        tgt_padding_mask, # padding mask for the target. It is already aligned as we build it from tgt_input\n",
        "        src_padding_mask  # memory key padding mask: mask the padding tokens during attention computation when the decoder attends to the encoder's output.\n",
        "        )\n",
        "\n",
        "    loss = loss_fn(logits.reshape(-1, logits.size(-1)), tgt_output.reshape(-1))\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss.item()\n",
        "\n",
        "\n",
        "def evaluate(model, val_dataloader):\n",
        "    model.eval()\n",
        "    losses = 0\n",
        "    total_loss = 0\n",
        "    tot_tokens = 0\n",
        "\n",
        "    for batch in val_dataloader:\n",
        "        src = batch['input_ids_src'].to(DEVICE)\n",
        "        tgt = batch['input_ids_tgt'].to(DEVICE)\n",
        "\n",
        "        # Teacher forcing\n",
        "        tgt_input = tgt[:, :-1,]\n",
        "        tgt_out = tgt[:, 1:]\n",
        "\n",
        "        # Compute the masks\n",
        "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
        "\n",
        "        logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
        "\n",
        "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
        "\n",
        "        # Count the target tokens ignoring padding index\n",
        "        num_tokens = (tgt_out != PAD_IDX).sum().item()\n",
        "\n",
        "        losses += loss.item() * num_tokens # To avoid bias when batches have different size\n",
        "        total_loss += loss.item()\n",
        "        tot_tokens += num_tokens\n",
        "\n",
        "    avg_loss = losses / tot_tokens\n",
        "    perplexity = math.exp(avg_loss)\n",
        "\n",
        "    return total_loss, perplexity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IgGe3597qGRg"
      },
      "source": [
        "Let's also write a couple functions to greedily decode sequences produced by the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "zREZRgNZp8gj"
      },
      "outputs": [],
      "source": [
        "def greedy_decode(model, src_tokens, src_mask, max_len, start_symbol, end_symbol, device):\n",
        "    model.eval()\n",
        "\n",
        "    # Encode the source\n",
        "    memory = model.encode(src_tokens, src_mask)\n",
        "\n",
        "    # Initialize target with BOS\n",
        "    ys = torch.tensor([[start_symbol]], dtype=torch.long, device=device)\n",
        "\n",
        "    for _ in range(max_len):\n",
        "        # Build autoregressive mask for the current target length\n",
        "        tgt_mask = torch.nn.Transformer.generate_square_subsequent_mask(ys.size(1), device)\n",
        "\n",
        "        # Decode\n",
        "        out = model.decode(ys, memory, tgt_mask)\n",
        "\n",
        "        # Get logits for last timestep\n",
        "        logits = model.generator(out[:, -1])     # shape (1, vocab)\n",
        "\n",
        "        # Greedy: pick best token\n",
        "        next_word = logits.argmax(dim=1).item()\n",
        "\n",
        "        # Append to output sequence\n",
        "        ys = torch.cat(\n",
        "            [ys, torch.tensor([[next_word]], dtype=torch.long, device=device)],\n",
        "            dim=1\n",
        "        )\n",
        "\n",
        "        # Stop if EOS\n",
        "        if next_word == end_symbol:\n",
        "            break\n",
        "\n",
        "    return ys\n",
        "\n",
        "def translate(model, sentence, src_tokenizer, tgt_tokenizer, device=DEVICE, max_len=100):\n",
        "    model.eval()\n",
        "\n",
        "    # Tokenize without padding\n",
        "    src_tokens = src_tokenizer(\n",
        "        sentence,\n",
        "        padding=False,\n",
        "        truncation=True,\n",
        "        return_tensors=\"pt\"\n",
        "    )[\"input_ids\"].to(device)\n",
        "\n",
        "    # Build src mask\n",
        "    src_mask = torch.zeros((src_tokens.size(1), src_tokens.size(1)),\n",
        "                           dtype=torch.bool, device=device)\n",
        "\n",
        "    # Greedy decoding\n",
        "    ys = greedy_decode(\n",
        "        model,\n",
        "        src_tokens,\n",
        "        src_mask,\n",
        "        max_len=max_len,\n",
        "        start_symbol=tgt_tokenizer.bos_token_id,\n",
        "        end_symbol=tgt_tokenizer.eos_token_id,\n",
        "        device=device\n",
        "    )\n",
        "\n",
        "    # Convert token IDs to string without taking into account special tokens (e.g., [PAD] or [BOS])\n",
        "    return tgt_tokenizer.decode(ys.squeeze().tolist(), skip_special_tokens=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kjk9Wbqk33tj"
      },
      "source": [
        "Write a function to qualitatively evaluate the quality of the translation of a single sentence, from the validation set, from the model. This is going to be used at the end of every training epoch to have a general idea of how / if the model is improving. Use the functions defined above within it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_7ONsGYF4YoD"
      },
      "outputs": [],
      "source": [
        "def evaluate_single_sentence():\n",
        "  pass #TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tCcywK3w4pHP"
      },
      "source": [
        "# Model training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GTyJx9VpKhCc"
      },
      "source": [
        "We finally have everything we need to train our Transformer model.\n",
        "\n",
        "Using all the functions defined above, train the transformer model for 20 epochs, keep track of training and validation losses at every epoch, as they will be useful to evaluate the model afterwards.\n",
        "\n",
        "Make sure to print out essential information during the training.\n",
        "For each epoch you should print out:\n",
        "\n",
        "- Epoch number\n",
        "- Training loss\n",
        "- Validation loss\n",
        "- Perplexity\n",
        "- A single source sentence\n",
        "- The corresponding target sentence\n",
        "- The translation of the model for that sentence\n",
        "- Optionally the amount of time for the epoch\n",
        "\n",
        "\n",
        "The training should take approximately 15-20 minutes (with Colab's GPU).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4zWk56V45vNN"
      },
      "outputs": [],
      "source": [
        "from timeit import default_timer as timer\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "NUM_EPOCHS = 20\n",
        "\n",
        "training_losses = # TODO\n",
        "val_losses = # TODO\n",
        "\n",
        "print(\"Untrained model\")\n",
        "source, target, translation = evaluate_single_sentence(model, 0, tokenizer_src, tokenizer_tgt)\n",
        "\n",
        "print(\"*\"*20)\n",
        "print(\"Training\")\n",
        "print(\"*\"*20)\n",
        "\n",
        "for epoch in range(1, NUM_EPOCHS+1):\n",
        "    start_time = timer()\n",
        "\n",
        "\n",
        "    # TODO\n",
        "\n",
        "\n",
        "    end_time = timer()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8dhZ_JVlC3j0"
      },
      "source": [
        "# Model evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2-W96Pl5_4K"
      },
      "source": [
        "As usual let's plot out the evolution of the training and validation losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KBeRpdrW7ZMj"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NC-oxu4G7ekz"
      },
      "source": [
        "# Test translation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XxaiUJFMEC9g"
      },
      "source": [
        "Let's test the translation with a few sentences, different conditions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53AQPVl7KhCd",
        "outputId": "f8c908c7-4f1b-41ef-e0d1-7de04bccc54f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "In dataset\n",
            "A group of people standing in front of an office, one of them.\n",
            "In appropriate context\n",
            "A red car driver shaving on the highway in red car.\n",
            "A young boy playing with a ball, who is wearing a tambourine.\n",
            "Out of domain\n",
            "The conference is at the conference table of the soldier is taking a discussion on the podium.\n",
            "Protesters are watching this merry - like they are in the urban pool, most of them are seen from the people watch near this team ' s company ' s company ' s team.\n"
          ]
        }
      ],
      "source": [
        "# For a sentence we know it's in the dataset\n",
        "print(\"In dataset\")\n",
        "print(translate(model, \"Eine Gruppe von Menschen steht vor einem Iglu .\", tokenizer_src, tokenizer_tgt)) # A group of people stands in front of an igloo.\n",
        "# For some sentences that are appropriate for the dataset type (image captions)\n",
        "print(\"In appropriate context\")\n",
        "print(translate(model, \"Ein rotes Auto rast auf der Autobahn\", tokenizer_src, tokenizer_tgt)) # A red car is racing on the highway\n",
        "print(translate(model, \"Ein kleiner Junge, der mit einem Ball spielt\", tokenizer_src, tokenizer_tgt)) # A little boy playing with a ball\n",
        "# Something completely different, a statement\n",
        "print(\"Out of domain\")\n",
        "print(translate(model, \"Der PrÃ¤sident des Verbandes kÃ¼ndigte eine Konferenz an\", tokenizer_src, tokenizer_tgt)) # The president of the association announced a conference\n",
        "# Something that could be from a messaging app\n",
        "print(translate(model, \"Wir sehen uns morgen um 18 Uhr in der NÃ¤he des stÃ¤dtischen Schwimmbades\", tokenizer_src, tokenizer_tgt)) # See you tomorrow at 6 p.m. near the municipal swimming pool"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dsC6j4fs6MGq"
      },
      "source": [
        "You can try also with your own test sentences:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "quXRAo3S6Kfp"
      },
      "outputs": [],
      "source": [
        "my_sentence = # TODO (note that this should be in German)\n",
        "print(translate(model, my_sentence, tokenizer_src, tokenizer_tgt))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kdg4Uwmd9Hvs"
      },
      "source": [
        "We've seen a lot in this exercise, but as you can see the model is limited. After all you wouldn't expect to solve machine translation on a laptop in 30 minutes, right?\n",
        "If you want you can try several things:\n",
        "- change minumum number of times a word has to appear in the training set to be included in the vocabulary\n",
        "- change tokenizers\n",
        "- change hyperparameters of the transformer architecture (**emb size**, **ffnn_dim**, n_enc, n_dec, ...)\n",
        "- change training hyperparameters (batch size, number of epochs)\n",
        "- implement other evaluation metrics (e.g. [BLEU score](https://en.wikipedia.org/wiki/BLEU))\n",
        "- implement a different way to generate sequences in inference (e.g.[ Beam search](https://towardsdatascience.com/foundations-of-nlp-explained-visually-beam-search-how-it-works-1586b9849a24))\n",
        "- You can also use this notebook as a starting point for a different machine translation dataset / task, the steps are approximately the same."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EUy4x8AYKhCd"
      },
      "source": [
        "References\n",
        "==========\n",
        "\n",
        "1. [Multi30K: Multilingual English-German Image Descriptions](https://arxiv.org/abs/1605.00459)\n",
        "2. Attention is all you need paper.\n",
        "    <https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf>\n",
        "3.  The annotated transformer.\n",
        "    <https://nlp.seas.harvard.edu/2018/04/03/attention.html#positional-encoding>\n",
        "4.  Blogpost transformers explained https://towardsdatascience.com/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452\n",
        "5.  Mikolov, T., Chen, K., Corrado, G., & Dean, J. (2013). Efficient estimation of word representations in vector space. <https://arxiv.org/abs/1301.3781>\n",
        "6.  Sutskever, I., Vinyals, O., & Le, Q. V. (2014). Sequence to sequence learning with neural networks. <https://proceedings.neurips.cc/paper/2014/hash/a14ac55a4f27472c5d894ec1c3c743d2-Abstract.html>\n",
        "7.  Bahdanau, D., Cho, K., & Bengio, Y. (2014). Neural machine translation by jointly learning to alignand translate. <https://arxiv.org/abs/1409.0473>\n",
        "8.  He, K., Zhang, X., Ren, S., & Sun, J. (2016). Deep residual learning for image recognition. <https://openaccess.thecvf.com/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html>\n",
        "9.  Ba, J. L., Kiros, J. R., & Hinton, G. E. (2016). Layer normalization. <https://arxiv.org/abs/1607.06450>\n",
        "10.  Hochreiter, S., & Schmidhuber, J. (1997). Long short-term memory <https://ieeexplore.ieee.org/abstract/document/6795963>\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
