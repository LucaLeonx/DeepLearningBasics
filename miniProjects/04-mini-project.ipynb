{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini Project 4\n",
    "\n",
    "By the deadline (**26.01.2026**), please submit the provided Jupyter notebook with all/some required tasks completed and clearly solved. Make sure your code is neat, well-commented, and that all outputs are visible (run all cells before saving). Notebooks with missing tasks or unexecuted cells may receive fewer points. After you submit, you won’t be able to make changes, so double-check your work and be sure to start from the provided template\n",
    "\n",
    "## Submission rules\n",
    "As already discussed in class, we will stick to the following rules.\n",
    "- Use the templates and name your files `NAME_SURNAME.ipynb` (If you have more than one name, just concatenate them). We will compare what you present with that file. \n",
    "- Code either not written in Python or not using PyTorch receives a grade of 0. Of course, you can use auxiliary packages when needed (`matplotlib`, `numpy`, `networkx`...), but for the learning part, you must use PyTorch.\n",
    "-  If plagiarism is suspected, TAs and I will thoroughly investigate the situation, and we will summon the student for a face-to-face clarification regarding certain answers they provided. In case of plagiarism, a score reduction will be applied to all the people involved, depending on their level of involvement.\n",
    "-  If extensive usage of AI tools is detected, we will summon the student for a face-to-face clarification regarding certain answers they provided. If the answers are not adequately supported with in-person answers, we will proceed to apply a penalty to the evaluation, ranging from 10% to 100%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traveling Salesperson Problem (TSP) – Heuristic with Transformers\n",
    "\n",
    "### Introduction\n",
    "\n",
    "The Traveling Salesperson Problem (TSP) is a classical combinatorial optimization problem: given a set of cities and the distances between them, the goal is to find the shortest possible tour that visits each city exactly once and returns to the starting point. See this picture for reference. \n",
    "\n",
    "![TSP](https://optimization.cbe.cornell.edu/images/e/ea/48StatesTSP.png)\n",
    "\n",
    "TSP is NP-hard, meaning that finding the optimal solution becomes computationally intractable as the number of cities grows. Therefore, in practice, we often rely on heuristic or approximate algorithms to find good solutions in a reasonable time.\n",
    "\n",
    "In this assignment, your task is to implement a heuristic TSP solver for Euclidean TSP using a Transformer-based model. \n",
    "\n",
    "Euclidean TSP is a TSP where all the cities are assumed to be in the Euclidean 2D space and the distance is computed acording to the standard Euclidean norm.\n",
    "\n",
    "The Transformer architecture, widely used in natural language processing, can also model sequences of nodes in a graph. By training a Transformer on TSP instances, you can generate tours in an autoregressive manner selecting one city at a time based on previously chosen cities.\n",
    "\n",
    "This assignment will help you to understand how sequence models like Transformers can be applied beyond text, to combinatorial problems and practice autoregressive generation and masking techniques for sequence prediction.\n",
    "\n",
    "Most of the code is provided. This **does not mean that you are allowed to use it blindly**: make sure to understand what is happening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-28T11:19:17.149032Z",
     "iopub.status.busy": "2025-11-28T11:19:17.148698Z",
     "iopub.status.idle": "2025-11-28T11:19:17.153997Z",
     "shell.execute_reply": "2025-11-28T11:19:17.153376Z",
     "shell.execute_reply.started": "2025-11-28T11:19:17.149001Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from networkx.algorithms.approximation import greedy_tsp\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1 (0 pts)\n",
    "\n",
    "Learn how to use the [NetworkX](https://networkx.org/documentation/stable/tutorial.html) package. The section on Graphs should be sufficient for this assignment. You can skip the sections on directed graphs and beyond.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility Functions\n",
    "\n",
    "Below are some utility functions. Detailed explanations are provided in each function’s description. At a high level, these functions do the following:\n",
    "\n",
    "* `random_tour`: Takes a `networkx.Graph` as input and returns a random tour. This can serve as a simple heuristic, but its performance is generally low, as expected.\n",
    "* `tour_length`: Takes a `networkx.Graph` and a tour (encoded as a `list` that starts and ends at the same node) and returns the total length of the tour.\n",
    "* `gap`: Measures the quality of a tour by computing\n",
    "\n",
    "$$ \\dfrac{\\text{Estimated} - \\text{Optimal}}{\\text{Optimal}} $$\n",
    "\n",
    "As you might expect, lower values are better, with a gap of zero indicating that the tour is exactly optimal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-11-25T14:38:03.856249Z",
     "iopub.status.busy": "2025-11-25T14:38:03.855737Z",
     "iopub.status.idle": "2025-11-25T14:38:03.863701Z",
     "shell.execute_reply": "2025-11-25T14:38:03.862624Z",
     "shell.execute_reply.started": "2025-11-25T14:38:03.856225Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def tour_length(G: networkx.Graph, tour: List[int]) -> float:\n",
    "    \"\"\"\n",
    "    Generate a random TSP tour for a given graph.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    G : networkx.Graph\n",
    "        A graph representing the TSP instance. Nodes should be numbered from 0 to n-1.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tour : list of int\n",
    "        A list of node indices representing a complete tour starting and ending at node 0.\n",
    "        Each node appears exactly once in the tour (except the start/end node).\n",
    "    \"\"\"\n",
    "    n = G.number_of_nodes()\n",
    "    tour = [0]\n",
    "    for i in range(1, n):\n",
    "        next_node = np.random.choice([j for j in range(n) if j not in tour])\n",
    "        tour.append(next_node)\n",
    "    tour.append(0)\n",
    "    return tour\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def gap(estimated : float, optimal : float) -> float:\n",
    "    \"\"\"\n",
    "    Compute the relative optimality gap of a TSP tour.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    estimated : float\n",
    "        The total cost of the computed tour.\n",
    "    optimal : float\n",
    "        The cost of the known optimal tour.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    gap : float\n",
    "        The relative gap between the estimated and optimal cost,\n",
    "        computed as (estimated - optimal) / optimal.\n",
    "    \"\"\"\n",
    "    return (estimated - optimal) / optimal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2 (10 pts)\n",
    "\n",
    "The dataset has already been prepared for your convenience and is split into training, validation, and test sets. Load it using [pickle](https://docs.python.org/3/library/pickle.html).\n",
    "\n",
    "For this task, focus on the case **$n = 20$**, i.e., solving a TSP with 20 cities. Inspect each item in the dataset: what type is it? You should see that it is a `tuple` consisting of a `networkx` graph and a list representing the optimal tour (our ground truth).\n",
    "\n",
    "Answer the following questions carefully:\n",
    "\n",
    "* What does the node attribute `pos` represent?\n",
    "* What does the edge attribute `weight` represent?\n",
    "* What does the boolean edge attribute `tour` represent?\n",
    "\n",
    "Do not proceed until you fully understand each of these components.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taks 3 (4 pts)\n",
    "\n",
    "In this task, you will implement a custom PyTorch Dataset to handle TSP instances. Your dataset should take as input a list of tuples, where each tuple contains:\n",
    "\n",
    "1. A `networkx` graph representing the cities and their distances.\n",
    "2. A list representing the optimal tour (ground truth).\n",
    "\n",
    "You should implement a class `TSPDataset` that returns, for a given index `idx`, a tuple `(X, opt_tour)` where:\n",
    "\n",
    "  * `X` is a tensor of shape `[n_nodes, 2]` containing the coordinates of each node (from the node attribute `pos`).\n",
    "  * `opt_tour` is a tensor containing the optimal tour as a sequence of node indices.\n",
    "\n",
    "Then, create a dataset object for both training, validation and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-25T14:38:03.931848Z",
     "iopub.status.busy": "2025-11-25T14:38:03.931490Z",
     "iopub.status.idle": "2025-11-25T14:38:03.936924Z",
     "shell.execute_reply": "2025-11-25T14:38:03.936002Z",
     "shell.execute_reply.started": "2025-11-25T14:38:03.931821Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class TSPDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data):\n",
    "        pass\n",
    "\n",
    "    def __len__(self):\n",
    "        pass\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4 (3 pts)\n",
    "\n",
    "Create the `Dataloader` objects for training and validation. Probably you just need training and validation loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5 (5 pts)\n",
    "\n",
    "Implement Positional encoding for sequence models. Positional encoding allows a Transformer to incorporate information about the order of elements in a sequence, since the model itself is permutation-invariant.\n",
    "\n",
    "You are asked to implement a function/a class that takes as input:\n",
    "\n",
    "  * `seq_len`: the length of the sequence (number of positions).\n",
    "  * `embed_dim`: the dimensionality of the embedding vectors.\n",
    "\n",
    "And returns a tensor of shape `[seq_len, embed_dim]`, where each row corresponds to the positional encoding vector for a position in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-25T14:46:41.503071Z",
     "iopub.status.busy": "2025-11-25T14:46:41.501593Z",
     "iopub.status.idle": "2025-11-25T14:46:41.511737Z",
     "shell.execute_reply": "2025-11-25T14:46:41.510617Z",
     "shell.execute_reply.started": "2025-11-25T14:46:41.503038Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Option 1\n",
    "def positional_encoding(seq_len, embed_dim):\n",
    "    pass\n",
    "\n",
    "# Option 2\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self,max_seq_len, embed_dim):\n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 6 (30 pts)\n",
    "\n",
    "In this task, you are asked to implement a Transformer-like architecture. You have two options:\n",
    "\n",
    "1. Implement the architecture provided in the pdf and sketched with guidance in the next cell. This option is fully guided, however, following it exactly will give you full points.\n",
    "\n",
    "2. Implement your own Transformer-based architecture. You are free to design it as you wish, as long as it is Transformer-based. No extra points will be awarded for choosing this option, but full points can still be earned if it works correctly.\n",
    "\n",
    "Choose the approach that best suits your preference and experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class TSPTransformer(nn.Module):\n",
    "    def __init__(self,\n",
    "        n: int,\n",
    "        d_model: int,\n",
    "        nhead_encoder: int,\n",
    "        nhead_decoder: int,\n",
    "        n_encoder_layers: int,\n",
    "        n_decoder_layers: int,\n",
    "        dropout: float = 0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Things to store, you need them also in the forward\n",
    "        self.d_model = d_model\n",
    "\n",
    "        ######################################\n",
    "        # Feature expansion\n",
    "        #####################################\n",
    "        # Linear layer\n",
    "\n",
    "        # Embedding\n",
    "\n",
    "        ######################################\n",
    "        # Encoder\n",
    "        #####################################\n",
    "        # Encoder layer\n",
    "\n",
    "        # Encoder\n",
    "\n",
    "        ######################################\n",
    "        # Output linear layer\n",
    "        #####################################\n",
    "        \n",
    "\n",
    "        ######################################\n",
    "        # Decoder\n",
    "        #####################################\n",
    "        \n",
    "        # Positional Embedding\n",
    "        \n",
    "        # Decoder layer\n",
    "\n",
    "        # Decoder\n",
    "        \n",
    "        \n",
    "        ######################################\n",
    "        # Output linear layer\n",
    "        #####################################\n",
    "        \n",
    "        # Don't forget the \"Masked\" attention!\n",
    "\n",
    "        # Initialize the weight as commonly done for the LLMs\n",
    "        self.apply(self.init_weights)\n",
    "\n",
    "    def forward(self, X, src):\n",
    "        pass\n",
    "        \n",
    "\n",
    "    def create_mask(self, seq_len):\n",
    "        pass\n",
    "        \n",
    "    def init_weights(self, m):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 7 (35 pts)\n",
    "\n",
    "Train your model. Plot training and validation loss, clearly showing no overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 8 (3 pts)\n",
    "\n",
    "Finally, evaluate your model as if it were a heuristic for solving the TSP. Compare its performance with other simple baselines, such as:\n",
    "\n",
    "* A random tour sampled uniformly at random\n",
    "* The greedy heuristic, where you start from a given city and repeatedly move to the closest unvisited one. Its theoretical performance guarantees are quite weak, but in practice it can sometimes work surprisingly well.\n",
    "\n",
    "The code needed to run these baselines is already provided in the next cells. There is just something you may want to check in the second cell after this one.\n",
    "Before using it, make sure you fully understand what each part of the provided code is doing.\n",
    "\n",
    "At the very end, you should see something like this, with our trained model beating all the baselines:\n",
    "\n",
    "```\n",
    "Greedy TSP average gap: 0.171123\n",
    "Random TSP average gap: 1.745515\n",
    "Our TSP average gap: 0.05374\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def transformer_tsp(G, model, start_node = 0, DEVICE=\"cpu\"):\n",
    "    \"\"\"\n",
    "    Evaluate a trained Transformer-based TSP heuristic on a given graph G.\n",
    "\n",
    "    This function performs greedy autoregressive decoding: starting from node 0,\n",
    "    it repeatedly queries the model to predict the next city to visit, always\n",
    "    selecting the highest-scoring unvisited node. The predicted tour is finally\n",
    "    closed by returning to the starting point.\n",
    "\n",
    "    Parameters\n",
    "    -----------\n",
    "        G (networkx.Graph):\n",
    "            A TSP instance where each node has a 'pos' attribute containing its\n",
    "            2D coordinates.\n",
    "        model (torch.nn.Module):\n",
    "            A trained Transformer-like neural network that takes:\n",
    "                - x: a tensor of node coordinates  \n",
    "                - y: the partial tour so far       \n",
    "              and outputs logits for the next node at each decoding step.\n",
    "        start_node (int):\n",
    "            The starting node; default is zero.\n",
    "        DEVICE (str, optional):\n",
    "            Device to run the model on (“cpu” or “cuda” (<-- GPU)). Default is \"cpu\".\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "        float:\n",
    "            The length of the predicted tour, computed using `tour_length`.\n",
    "    \"\"\"\n",
    "\n",
    "    # Set the model in evaluation mode (important for layers like dropout)\n",
    "    model.eval()\n",
    "\n",
    "    # Number of nodes\n",
    "    n = G.number_of_nodes()\n",
    "\n",
    "    # Extract node coordinates into a tensor of shape [n, 2]\n",
    "    attr = nx.get_node_attributes(G, \"pos\")\n",
    "    x = []\n",
    "    for i in range(n):\n",
    "        x.append(torch.tensor(attr[i], dtype=torch.float32))\n",
    "    x = torch.stack(x)\n",
    "\n",
    "    # Initial tour starts at node 0\n",
    "    tour = [start_node]\n",
    "    y = torch.tensor(tour, dtype=torch.long)\n",
    "\n",
    "    # Move tensors to the desired device and add batch dimension\n",
    "    x = x.to(DEVICE).unsqueeze(0)\n",
    "    y = y.to(DEVICE).unsqueeze(0)\n",
    "\n",
    "    # First forward pass\n",
    "    out = model(x, y)\n",
    "\n",
    "    # Autoregressive greedy decoding\n",
    "    while len(tour) < n:\n",
    "        # Get nodes sorted by descending score\n",
    "        _, idx = torch.topk(out, n, dim=2)\n",
    "\n",
    "        # Pick the highest-scoring unvisited node\n",
    "        for i in range(n):\n",
    "            if idx[0, -1, i] not in tour:\n",
    "                tour.append(idx[0, -1, i])\n",
    "                break\n",
    "\n",
    "        # Update decoder input and run model again\n",
    "        y = torch.tensor(tour, dtype=torch.long).to(DEVICE).unsqueeze(0)\n",
    "        out = model(x, y)\n",
    "\n",
    "    # Close the tour by returning to the starting node\n",
    "    tour = [int(i) for i in tour] + [start_node]\n",
    "\n",
    "    # Compute and return its length\n",
    "    return tour_length(G, tour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "TODO\n",
    "1. Load / Use the model you want to use (I guess, your best/last model)\n",
    "2. Have you loaded the training data?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model = None\n",
    "X_test = None\n",
    "\n",
    "optimal_tours_values = []\n",
    "greedy_tsp_values = []\n",
    "random_tsp_values = []\n",
    "our_heuristic_values = []\n",
    "\n",
    "for idx, (G, y_opt) in tqdm(enumerate(X_test), total=len(X_test)):\n",
    "    # OPT\n",
    "    optimal_tours_values.append(tour_length(G, y_opt))\n",
    "\n",
    "    # Greedy TSP\n",
    "    tour_greedy = greedy_tsp(G)\n",
    "    greedy_tsp_values.append(tour_length(G, tour_greedy))\n",
    "\n",
    "    # Random TSP\n",
    "    tour_random = random_tour(G)\n",
    "    random_tsp_values.append(tour_length(G, tour_random))\n",
    "\n",
    "    # Our Heuristic\n",
    "    our_heuristic_values.append(transformer_tsp(G, model, DEVICE=DEVICE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Greedy TSP gap\n",
    "gaps_greedy = []\n",
    "for i in range(len(X_test)):\n",
    "    gap_greedy = gap(greedy_tsp_values[i], optimal_tours_values[i])\n",
    "    gaps_greedy.append(gap_greedy)\n",
    "print(\"Greedy TSP average gap:\", np.mean(gaps_greedy), flush=True)\n",
    "\n",
    "# Random TSP gap\n",
    "gaps_random = []\n",
    "for i in range(len(X_test)):\n",
    "    gap_random = gap(random_tsp_values[i], optimal_tours_values[i])\n",
    "    gaps_random.append(gap_random)\n",
    "print(\"Random TSP average gap:\", np.mean(gaps_random), flush=True)\n",
    "\n",
    "# Our Heuristic\n",
    "gaps_our = []\n",
    "for i in range(len(X_test)):\n",
    "    gap_our = gap(our_heuristic_values[i], optimal_tours_values[i])\n",
    "    gaps_our.append(gap_random)\n",
    "print(\"Our TSP average gap:\", np.mean(gaps_our), flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions\n",
    "\n",
    "1. Can you use this model for a TSP instance with 50 nodes? How can you improve it?\n",
    "2. What is attention and why is it the core mechanism of Transformers?\n",
    "3. How does multi-head attention differ from single-head attention? Why is it useful?\n",
    "4. What is the purpose of positional encodings in a Transformer? Why are they needed?\n",
    "5. Describe the role of the feed-forward network inside each Transformer layer.\n",
    "6. Why do Transformers use Layer Normalization instead of Batch Normalization?\n",
    "7.  What are Q, K, and V? Explain their dimensions intuitively.\n",
    "8. Why do we scale the dot product by √dₖ in scaled dot-product attention?\n",
    "9. What is the intuition behind causal masking?\n",
    "11. What is the difference between training-time input and inference-time input in autoregressive models? What is teacher forcing? Why is it used?\n",
    "12. What is tokenization and why does it matter for LLM performance?\n",
    "13. What is the difference between pretraining and finetuning?\n",
    "14. Explain greedy decoding vs. sampling vs. beam search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31192,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "DeepLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
