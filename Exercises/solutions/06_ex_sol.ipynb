{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "398bef94",
   "metadata": {},
   "source": [
    "# Transfer learning: Fine-tuning of DistilGPT2 on Recipes dataset\n",
    "\n",
    "<img src=\"https://jalammar.github.io/images/xlnet/transformer-decoder-intro.png\" width=900 height=450>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b701bef2",
   "metadata": {},
   "source": [
    "### Kaggle troubleshooting\n",
    " \n",
    "If you plan to run the notebook on Kaggle, and cannot run the `load_dataset` function, try to run these commands as it seems it doesn't recognize `datasets` and `transformers`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb5c7735-0c48-420c-a5d2-dff37d4c2395",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-27T15:48:39.125538Z",
     "iopub.status.busy": "2025-11-27T15:48:39.124710Z",
     "iopub.status.idle": "2025-11-27T15:48:40.737279Z",
     "shell.execute_reply": "2025-11-27T15:48:40.736539Z",
     "shell.execute_reply.started": "2025-11-27T15:48:39.125505Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence-transformers==4.1.0\n",
      "transformers==4.53.3\n"
     ]
    }
   ],
   "source": [
    "!pip freeze | grep \"transformers\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8d37102-e4b9-446e-9fc0-14e6618b217d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-27T15:48:57.302811Z",
     "iopub.status.busy": "2025-11-27T15:48:57.302211Z",
     "iopub.status.idle": "2025-11-27T15:48:58.937175Z",
     "shell.execute_reply": "2025-11-27T15:48:58.936423Z",
     "shell.execute_reply.started": "2025-11-27T15:48:57.302791Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasets==4.4.1\n",
      "tensorflow-datasets==4.9.9\n",
      "vega-datasets==0.9.0\n"
     ]
    }
   ],
   "source": [
    "!pip freeze | grep \"datasets\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3c8ab0",
   "metadata": {},
   "source": [
    "# Load the dataset and split it into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4586a50a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-27T15:48:58.938527Z",
     "iopub.status.busy": "2025-11-27T15:48:58.938213Z",
     "iopub.status.idle": "2025-11-27T15:48:59.670838Z",
     "shell.execute_reply": "2025-11-27T15:48:59.670292Z",
     "shell.execute_reply.started": "2025-11-27T15:48:58.938494Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luca/miniconda3/envs/DeepLearning/lib/python3.14/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Generating train split: 100%|██████████| 2147248/2147248 [00:02<00:00, 857533.05 examples/s] \n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n",
    "\n",
    "dataset = load_dataset(\"corbt/all-recipes\", split=\"train[:10000]\") # Out of more than 2M rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5879e219",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-27T15:49:06.449673Z",
     "iopub.status.busy": "2025-11-27T15:49:06.449368Z",
     "iopub.status.idle": "2025-11-27T15:49:06.467130Z",
     "shell.execute_reply": "2025-11-27T15:49:06.466349Z",
     "shell.execute_reply.started": "2025-11-27T15:49:06.449648Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input'],\n",
       "        num_rows: 8000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0814c703",
   "metadata": {},
   "source": [
    "Let's check an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dacb0ecb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-27T15:49:06.703685Z",
     "iopub.status.busy": "2025-11-27T15:49:06.703118Z",
     "iopub.status.idle": "2025-11-27T15:49:12.026691Z",
     "shell.execute_reply": "2025-11-27T15:49:12.025754Z",
     "shell.execute_reply.started": "2025-11-27T15:49:06.703658Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chicken Pizza\n",
      "\n",
      "Ingredients:\n",
      "- 1 (8 oz.) pkg. refrigerated crescent dinner rolls\n",
      "- 2 whole chicken breasts, split, skinned and boned\n",
      "- 1/4 c. vegetable oil\n",
      "- 1 large onion, sliced into thin rings\n",
      "- 1 large green bell pepper, sliced into thin rings\n",
      "- 1/2 lb. fresh mushrooms, sliced\n",
      "- 1/2 c. pitted ripe olives, sliced\n",
      "- 1 (10 1/2 oz.) can pizza sauce with cheese\n",
      "- 1 tsp. garlic salt\n",
      "- 1 tsp. dried oregano\n",
      "- 1/4 c. grated Parmesan cheese\n",
      "- 2 c. (8 oz.) shredded Mozzarella cheese\n",
      "\n",
      "Directions:\n",
      "- Separate rolls into 8 triangles.\n",
      "- Press triangles into lightly oiled 12-inch pizza pan, covering it completely.\n",
      "- Cut up chicken into bite size pieces.\n"
     ]
    }
   ],
   "source": [
    "print(dataset['train']['input'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35e38c2",
   "metadata": {},
   "source": [
    "Load the model and the corresponding tokenizer.\n",
    "\n",
    "**NOTE**: Trained models work with their own tokenizer. If you plan to train a model from scratch (as in `exercise 5`), you can use any tokenizer (even pre-trained ones), but make sure to resize the token embedding matrices of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8407d90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the token is already in the vocabulary\n",
    "model_name = \"distilbert/distilgpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca737777",
   "metadata": {},
   "source": [
    "Let's tokenize the dataset by taking advantage of the `map` method of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b4e0a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 8000/8000 [00:00<00:00, 13350.13 examples/s]\n",
      "Map: 100%|██████████| 2000/2000 [00:00<00:00, 13411.84 examples/s]\n",
      "Map: 100%|██████████| 8000/8000 [00:00<00:00, 16918.02 examples/s]\n",
      "Map: 100%|██████████| 2000/2000 [00:00<00:00, 17960.84 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def tokenize_batch(examples):\n",
    "    return tokenizer(examples['input'], padding=True, truncation=True, max_length=128)\n",
    "\n",
    "# TODO: tokenize the datasets.Dataset properly and return PyTorch tensors for Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff77df30",
   "metadata": {},
   "source": [
    "# Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db12f063",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_batch(examples):\n",
    "    return tokenizer(examples['input'], padding=True, truncation=True, max_length=128)\n",
    "\n",
    "# As the padding token is a special token, we can check the attribute `special_tokens_map` to verify whether it is present or not.\n",
    "if 'eos_token' in tokenizer.special_tokens_map:\n",
    "    print(True)\n",
    "else:\n",
    "    print('The special token `EOS` is not present!')\n",
    "    print(f\"Length of the vocabulary: {len(tokenizer.get_vocab())}\")\n",
    "    print('Adding the special token `EOS` to the vocabulary')\n",
    "    tokenizer.add_special_tokens({'pad_token':'EOS'})\n",
    "    print(f\"Length of the new vocabulary: {len(tokenizer.get_vocab())}\")\n",
    "    print(f\"Added vocabulary: {tokenizer.get_added_vocab()}\")\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Tokenize the datasets.Dataset properly and return PyTorch tensors for Trainer\n",
    "tokenized_ds = dataset.map(tokenize_batch, batched=True, remove_columns=dataset['train'].column_names)\n",
    "# provide labels for Trainer (causal LM uses input_ids as labels)\n",
    "tokenized_ds = tokenized_ds.map(lambda x: {\"labels\": x[\"input_ids\"]}, batched=True) # Not necessary if passing mlm = False in data collator, but keep it for clarity\n",
    "tokenized_ds.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0d6a45",
   "metadata": {},
   "source": [
    "Let's add the [Data Collator](https://huggingface.co/docs/transformers/main_classes/data_collator) for dynamic padding. \n",
    "\n",
    "Import the `DataCollatorForLanguageModeling` and instantiate it.\n",
    "\n",
    "**NOTE**: In <u>Causal Language Modeling</u>, we don't need the padding token. We will pad all the sequences with the end of sequence token (e.g., `EOS`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e212986",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Import\n",
    "\n",
    "# Check if the tokenizer has the eos token\n",
    "\n",
    "# Set the padding token equal to the eos token\n",
    "\n",
    "# Instantiate the data collator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86220739",
   "metadata": {},
   "source": [
    "# Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b95df5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e193fc1",
   "metadata": {},
   "source": [
    "Let's see how the model behaves before the fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6697776",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-27T15:49:23.688494Z",
     "iopub.status.busy": "2025-11-27T15:49:23.688228Z",
     "iopub.status.idle": "2025-11-27T15:49:24.710441Z",
     "shell.execute_reply": "2025-11-27T15:49:24.709648Z",
     "shell.execute_reply.started": "2025-11-27T15:49:23.688471Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Pancakes\n",
      "Ingredients:\n",
      "1/2 cup of water\n",
      "1/2 cup of water\n",
      "1/2 cup of water\n",
      "1/2 cup of water\n",
      "1/2 cup of water\n",
      "1/2 cup of water\n",
      "1/2 cup of water\n",
      "1/2\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "\n",
    "model.to(DEVICE)\n",
    "\n",
    "prompt = \"Title: Pancakes\\nIngredients:\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "outputs = model.generate(**inputs, max_length=60)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd2372c",
   "metadata": {},
   "source": [
    "It seems we have to hydrate a lot. \n",
    "\n",
    "<img src=\"https://i.pinimg.com/originals/8e/fb/0c/8efb0cf59450a32e466848c5560d910d.png\" width=150 height=150>\n",
    "\n",
    "\n",
    "Let's see if after the fine-tuning!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb606e7",
   "metadata": {},
   "source": [
    "# Fine-tuning\n",
    "\n",
    "Run the fine-tuning on the given dataset for 10 epochs. Consider that a training of 10 epochs with a batch size of 64 recipes took ~30' on Kaggle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff362c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the training arguments\n",
    "args = TrainingArguments(\n",
    "    # TODO: \n",
    "    # Train for 10 epochs,\n",
    "    # use the epoch as evaluation strategy,\n",
    "    # Train with a batch size of 64. NOTE: If you plan to train it locally, make sure to have enough vRAM! If this is not enough, consider using the parameter `gradient_accumulation_steps`\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=100,\n",
    "    save_steps=200,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "# Instantiate the Trainer by passing the model, the datasets, the collator, and the arguments we have above\n",
    "trainer = Trainer(model=model, \n",
    "                  train_dataset=tokenized_ds['train'],\n",
    "                  eval_dataset=tokenized_ds['test'],\n",
    "                  data_collator=data_collator,\n",
    "                  args=args)\n",
    "# Train (easy right?)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd393fb",
   "metadata": {},
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b174bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-27T15:49:24.711969Z",
     "iopub.status.busy": "2025-11-27T15:49:24.711307Z",
     "iopub.status.idle": "2025-11-27T16:05:41.116153Z",
     "shell.execute_reply": "2025-11-27T16:05:41.115562Z",
     "shell.execute_reply.started": "2025-11-27T15:49:24.711948Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define the training arguments\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./fine_tuning/reGiPT2\",\n",
    "    num_train_epochs=10,\n",
    "    eval_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=64,\n",
    "    gradient_accumulation_steps=1,\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=100,\n",
    "    save_steps=200,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "# Instantiate the Trainer by passing the model, the datasets, the collator, and the arguments we have above\n",
    "trainer = Trainer(model=model, \n",
    "                  train_dataset=tokenized_ds['train'],\n",
    "                  eval_dataset=tokenized_ds['test'],\n",
    "                  data_collator=data_collator,\n",
    "                  args=args)\n",
    "# Train (easy right?)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567907aa",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "Let's measure the perplexity and study the `generate` method. By passing the parameters, you can change different things such as the decoding algorithm (i.e., greedy decoding, or beam search) or the temperature.\n",
    "\n",
    "**NOTE**: The temperature is a strictly positive float used to control how deterministic or random is the  model's output. A temperature value < 1 will output most probable tokens, while a larger value (i.e., > 1), will sample from tokens with a lower probability \n",
    "\n",
    "### Beam search visualization reminder\n",
    "<img src=https://pytorch.org/assets/images/fast-beam-search-decoding-in-pytorch-with-torchaudio-and-flashlight-text-1.jpeg>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6f0b7ae6-990b-4ec8-bfa4-6b4eb24f7122",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-27T16:05:41.117048Z",
     "iopub.status.busy": "2025-11-27T16:05:41.116840Z",
     "iopub.status.idle": "2025-11-27T16:05:49.914091Z",
     "shell.execute_reply": "2025-11-27T16:05:49.913438Z",
     "shell.execute_reply.started": "2025-11-27T16:05:41.117032Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [250/250 00:08]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 4.72\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abc4fef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-27T16:12:37.739534Z",
     "iopub.status.busy": "2025-11-27T16:12:37.738746Z",
     "iopub.status.idle": "2025-11-27T16:12:39.915370Z",
     "shell.execute_reply": "2025-11-27T16:12:39.914574Z",
     "shell.execute_reply.started": "2025-11-27T16:12:37.739501Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model.to(DEVICE)\n",
    "\n",
    "prompt = \"Title: Pancakes\\n\\nIngredients:\"\n",
    "print(prompt)\n",
    "print(30*'-')\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "# TODO: generate the output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742f0b32",
   "metadata": {},
   "source": [
    "# Solution\n",
    "\n",
    "You can check the available generation approaches at this [link](https://huggingface.co/docs/transformers/v4.57.3/generation_strategies)\n",
    "\n",
    "**NOTE**: the temperature parameter is taken into account only if we sample during the decoding (i.e., we have to set `do_sample` to `True`). You can check the [documentation](https://github.com/huggingface/transformers/blob/d08b98b965176ea9cf8c8e8b24995c955b7e2ec9/src/transformers/generation/logits_process.py#L244)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c380e7",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model.to(DEVICE)\n",
    "\n",
    "prompt = \"Title: Pancakes\\n\\nIngredients:\"\n",
    "print(\"PROMPT:\\n\",prompt)\n",
    "print(30*'-')\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "\n",
    "# output greedy, the temperature doesn't affect the greedy decoding\n",
    "outputs = model.generate(**inputs, max_length=256)\n",
    "print(\"Greedy decoding:\\n\")\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "print(30*'-')\n",
    "\n",
    "# output beam search decoding num_beams = 5, in this case, the temperature doesn't modify the \n",
    "outputs = model.generate(**inputs, max_length=256, num_beams=5, temperature=1.0)\n",
    "print(\"Beam search:\\n\")\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "print(30*'-')\n",
    "\n",
    "# output beam search multinomial sampling num_beams = 5, T = 1.0\n",
    "outputs = model.generate(**inputs, max_length=256, do_sample=True, num_beams=5, temperature=1.0)\n",
    "print(\"Beam search with multinomial sampling and temperature = 1:\\n\")\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "print(30*'-')\n",
    "\n",
    "# output beam search multinomial sampling num_beams = 5, T = 2.5\n",
    "outputs = model.generate(**inputs, max_length=256, do_sample=True, num_beams=5, temperature=2.5)\n",
    "print(\"Beam search with multinomial sampling and temperature = 2:\\n\")\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "print(30*'-')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25eba13",
   "metadata": {},
   "source": [
    "This notebook was based on the [causal language modeling tutorial on HuggingFace](https://huggingface.co/docs/transformers/tasks/language_modeling)\n",
    "\n",
    "Useful resources:\n",
    "- [Illustrated GPT2](https://jalammar.github.io/illustrated-gpt2/)\n",
    "- [Generation strategies](https://huggingface.co/docs/transformers/generation_strategies)\n",
    "- [Speech and Language Processing](https://web.stanford.edu/~jurafsky/slp3/) chapters 7 and 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66cbf717-394f-4c98-b195-072bf2c53d22",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 31192,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "DeepLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
