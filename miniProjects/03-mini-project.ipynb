{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Mini Project 3\n\nBy the deadline, please submit the provided Jupyter notebook with all/some required tasks completed and clearly solved. Make sure your code is neat, well-commented, and that all outputs are visible (run all cells before saving). Notebooks with missing tasks or unexecuted cells may receive fewer points. After you submit, you won‚Äôt be able to make changes, so double-check your work and be sure to start from the provided template\n\n## Submission rules\nAs already discussed in class, we will stick to the following rules.\n- Use the templates and name your files `NAME_SURNAME.ipynb` (If you have more than one name, just concatenate them). We will compare what you present with that file. \n- Code either not written in Python or not using PyTorch receives a grade of 0. Of course, you can use auxiliary packages when needed (`matplotlib`, `numpy`, ...), but for the learning part, you must use PyTorch.\n-  If plagiarism is suspected, TAs and I will thoroughly investigate the situation, and we will summon the student for a face-to-face clarification regarding certain answers they provided. In case of plagiarism, a score reduction will be applied to all the people involved, depending on their level of involvement.\n-  If extensive usage of AI tools is detected, we will summon the student for a face-to-face clarification regarding certain answers they provided. If the answers are not adequately supported with in-person answers, we will proceed to apply a penalty to the evaluation, ranging from 10% to 100%.","metadata":{}},{"cell_type":"markdown","source":"## Sentiment Analysis with LSTM\n\nThe IMDb dataset is a large collection of movie reviews compiled by the Internet Movie Database (IMDb), one of the most comprehensive online databases for films, TV shows, actors, and production crew information. IMDb is widely used for accessing details such as cast lists, user ratings, reviews, and plot summaries [(source)](https://www.geeksforgeeks.org/data-science/imdb-datasets-types-usages-and-application/).\n\nIn this mini-project, you will perform **sentiment analysis** on IMDb movie reviews using **LSTM-based models**. The goal is to classify each review as positive or negative.\n\nYou are required to build and train:\n\n* A **simple LSTM model**, aiming for at least **75% test accuracy**\n* A **more advanced LSTM model**, trying to push the accuracy as high as possible\n* A **function** that can evaluate the sentiment of any new review","metadata":{}},{"cell_type":"code","source":"# Packages here \nfrom datasets import load_dataset\nimport torch\nfrom torch.utils.data import DataLoader\nfrom torch.nn.utils.rnn import pad_sequence\nimport numpy as np\n\n# Don't change this \ntorch.manual_seed(123)\ntorch.cuda.manual_seed(123)\nnp.random.seed(123)\ntorch.backends.cudnn.enabled=False\ntorch.backends.cudnn.deterministic=True","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T12:50:12.174741Z","iopub.execute_input":"2025-11-14T12:50:12.175041Z","iopub.status.idle":"2025-11-14T12:50:12.182107Z","shell.execute_reply.started":"2025-11-14T12:50:12.175025Z","shell.execute_reply":"2025-11-14T12:50:12.180968Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"### Task 1 (0 pts)\n\nTo load the data, we will use [Hugging Face](https://huggingface.co/), an open-source platform that provides datasets, pre-trained models, and tools for modern machine learning.\n**Note:** In principle, you could also use [TorchText](https://docs.pytorch.org/text/main/datasets.html#imdb), but it does not work reliably on Kaggle. Using PyTorch‚Äôs dataset instead of Hugging Face is **not** considered an error.\n\nInspect the dataset you load and make sure you understand its structure and format.","metadata":{}},{"cell_type":"code","source":"ds = load_dataset(\"stanfordnlp/imdb\")\ndataset_train = ds['train']\ndataset_test = ds['test']","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Task 2 (5 pts)\n\nSplit the original **test set** into two parts: a **validation set** and a **final test set**. Use a split of 50-50.\n\n**Note:** We are aware that, in a typical machine learning workflow, you would split the **training set** (not the test set) to create a validation set. However, in this exercise, we intentionally use the test set for this purpose to provide you with **more training data** for model learning.","metadata":{}},{"cell_type":"code","source":"# TODO","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Task 3 (10 pts)\n\nCreate a `tokenize` function that takes a line from your dataset (a review) and converts it into tokens.\nYou may want to consider the following:\n\n* Are `film` and `film.` considered different?\n* Does it matter if a word is uppercase or lowercase?\n* Should tokens like `<br />` be included or removed?","metadata":{}},{"cell_type":"code","source":"def tokenize(line):\n    pass","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Task 4 (5 pts)\nCreate two dictionaries, `word_to_idx` and `idx_to_word`, to map words to their embedding indices and vice versa. At this stage, you should decide whether to remove outlier words and replace them with the `<UNK>` token. This decision will, of course, depend on the performance you observe afterward.","metadata":{}},{"cell_type":"code","source":"word_to_idx = {'<PAD>': 0, '<UNK>': 1}\nidx_to_word = {0: '<PAD>', 1: '<UNK>'}\n\n# TODO","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Task 5 (10 pts)\nCreate a `Dataset` class. The `__getitem__` function should return `(X, y)`, where `X` is a tensor containing the indices of the embedded words, and `y` is a tensor representing the sentiment expressed in the review.\n","metadata":{}},{"cell_type":"code","source":"class DatasetIMDB(torch.utils.data.Dataset):\n    def __init__(self, dataset, word_to_idx):\n        pass\n\n    def __len__(self):\n        pass\n\n    def __getitem__(self, idx):\n        pass","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Task 6 (10 pts)\n\nDefine a `collate` function for your DataLoader that ensures all sequences in a batch have the same length. The function should pad shorter sequences with the `<PAD>` index, so that every sequence in the batch matches the length of the longest sequence.","metadata":{}},{"cell_type":"code","source":"def collate_fn(batch):\n    pass","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Task 7 (5 pts)\n\nCreate one `DataLoader` for each dataset: Training, Validation, and Test. Make sure each `DataLoader` uses your `Dataset` class and the `collate_fn` function you defined in Task 6.","metadata":{}},{"cell_type":"code","source":"# TODO","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Task 8 (15 pts)\n\nDefine an `LSTM` class that can be customized as needed. Follow the provided template, but feel free to add additional attributes or methods if necessary.","metadata":{}},{"cell_type":"code","source":"class LSTM(torch.nn.Module):\n    def __init__(self, vocab_size, emb_dim=100, hidden_dim=128, num_layers=1, dropout=0, bidirectional=False):\n        super().__init__()\n        # ...\n        # if bidirectional....\n        pass\n\n    def forward(self, x):\n        # ...\n        # if bidirectional....\n        pass","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Task 9 (30 pts)\n\nTrain your model. Aim for a **test** accuracy of at least 75%. Be prepared to answer, among others, the following questions:\n\n* Did you use a stacked LSTM? Why or why not?\n* Did you use a bidirectional LSTM? Why or why not?\n* Did you need to adjust tensor dimensions for your model?\n* Which loss function did you choose, and why?\n\nPlot the training and evaluation losses, making sure there are no signs of overfitting, and print the final tarining/validation/test accuracy.","metadata":{}},{"cell_type":"code","source":"# TODO","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Task 10 (5 pts)\n\nImprove your model architecture and training procedure by applying one or more of the following strategies:\n\n* Truncated Backpropagation Through Time\n* Better tokenizer\n* Pre-trained embeddings\n* ....\n\nThis exercise is considered successful if **any \"sensible\" improvement in test accuracy** is achieved, even by applying just one change.","metadata":{}},{"cell_type":"code","source":"# TODO","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Task 11 (5 pts)\n\nUse one of your trained models to perform sentiment analysis on the following reviews. Be prepared to explain any issues you encountered and how you addressed them.","metadata":{}},{"cell_type":"code","source":"\"\"\"\nIron Man isn't just a superhero movie. It's the spark that ignited the entire Marvel Cinematic Universe. With Robert Downey Jr.'s career-defining performance, razor-sharp writing, and a perfect blend of heart, humor, and high-tech spectacle, this film redefined what a comic book movie could be.\n\nEven after more than a decade, Iron Man remains one of the most re-watchable, charming, and influential superhero origin stories ever made.\n\nüé¨ Overview\n\nIron Man introduces Tony Stark (Robert Downey Jr.), a brilliant but arrogant billionaire weapons manufacturer. When he's captured by terrorists in Afghanistan and forced to build a missile, Tony instead constructs a powered suit of armor to escape.\n\nHaunted by the destruction caused by his weapons, Stark returns home determined to reinvent himself. Not as a war profiteer, but as Iron Man, a hero powered by his mind, conscience, and an arc reactor in his chest.\n\nAlong the way, he faces betrayal from within his company, moral dilemmas, and a growing awareness of what true responsibility means.\n\n‚úÖ What Worked\n\n1. Robert Downey Jr.: Perfect casting. His charisma, wit, and emotional depth made Tony Stark unforgettable.\n\n2. Grounded realism: The technology feels just believable enough to make Iron Man's world plausible.\n\n3. Sharp writing and humor: Smart, fast-paced dialogue that balances action with personality.\n\n4. Emotional arc: Tony's transformation from egocentric arms dealer to self-aware hero feels authentic.\n\n5. Cinematography & effects: The first suit build and flight sequences are still jaw-dropping.\n\n6. Gwyneth Paltrow & Jeff Bridges: Excellent supporting cast. Pepper Potts' loyalty and Obadiah Stane's menace both shine.\n\n7. The ending line: That bold, improvised moment \"I am Iron Man.\" changed superhero cinema forever.\n\n‚ùå What Didn't Work\n\n1. Final battle pacing: The showdown between Iron Man and Iron Monger feels slightly rushed.\n\n2. Limited female representation: Pepper is great, but she's one of very few women in a male-dominated cast.\n\n3. Predictable villain motivation: Stane's greed is a bit by-the-numbers.\n\nBut honestly. These are small cracks in an otherwise near-perfect armor.\n\nüí¨ Favorite Quotes / Moments\n\n\"I am Iron Man.\" - Tony Stark \"Sometimes you've gotta run before you can walk.\" - Tony Stark \"My turn.\" - Tony, before launching a missile at a tank \"Is it better to be feared or respected? I say, is it too much to ask for both?\" - Tony Stark Tony's first cave suit escape: gritty, powerful, and unforgettable.\n\nThe Mark II flight test: pure cinematic joy as Tony takes to the skies.\n\nPepper replacing Tony's arc reactor: both funny and intimate.\n\nThe press conference ending: Tony discarding the superhero secrecy trope in one iconic line.\n\nThe post-credit scene: Nick Fury's appearance teasing the Avengers Initiative (and the MCU as we know it).\n\nüí° Fun Facts\n\n1. Robert Downey Jr. Was not Marvel's first choice but his audition blew everyone away.\n\n2. Much of the film's dialogue was improvised, including \"I am Iron Man.\"\n\n3. The movie was Marvel Studios' first independent production, made before Disney bought them.\n\n4. Jon Favreau insisted on using practical effects for the suits wherever possible.\n\n5. The success of Iron Man directly led to the creation of the MCU, which has since become the highest-grossing franchise in film history.\n\n6. Tony Stark's mansion was CGI. It doesn't actually exist!\n\nüé• If You Liked This, You Might Also Enjoy\n\n1. The Dark Knight (2008): Another intelligent and grounded superhero reboot.\n\n2. Iron Man 2 (2010): The next chapter in Tony Stark's evolution.\n\n3. Doctor Strange (2016): A spiritual successor exploring genius and redemption.\n\n4. Captain America: The First Avenger (2011): Marvel's other essential origin story.\n\n5. The Social Network (2010): Not a superhero movie, but a study of brilliance and ego that echoes Stark's early character.\n\nFinal Thoughts\n\nIron Man remains the gold standard for superhero origin stories. A perfect fusion of innovation, attitude, and emotion. It's not just about a man in a suit; it's about a man who learns to use his mind and heart for something greater.\n\nFinal Verdict: (9.5/10). \"The birth of Iron Man was also the birth of a cinematic universe and it still flies higher than ever.\"\n\"\"\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Questions\n\nDuring the presentation, we may ask questions to ensure you have understood the core concepts of the course. Examples include:\n\n1.\tWhat is the hidden state in a recurrent neural network (RNN), and what role does it play during sequence processing?\n2.\tWhy do we need padding when working with batches of variable-length sequences? How is padding typically handled in practice?\n3.\tWhat is a stacked RNN, and why might stacking multiple recurrent layers improve performance?\n4.\tWhat is a bidirectional RNN, and in which scenarios does it provide an advantage?\n5.\tWhat is the exploding gradient problem in recurrent networks?\n6.\tWhat is the vanishing gradient problem, and why is it particularly severe in RNNs?\n7.\tWhat is gradient clipping, and why is it commonly applied when training RNNs?\n8.\tHow do different activation functions influence vanishing or exploding gradients in deep or recurrent networks?\n9.\tWhat is truncated backpropagation through time, and why is it used when training RNNs?\n10.\tWhat is one-hot encoding for representing words in a vocabulary? What are its limitations?\n11.\tWhat does `nn.Embedding` do, and why is it preferred over one-hot encoding?\n12.\tWhat are gating mechanisms in recurrent architectures (e.g., LSTM/GRU), and why are they important?\n\t13.\tHow do LSTM gates help mitigate the vanishing gradient problem?","metadata":{}}]}